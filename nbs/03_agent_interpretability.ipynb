{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e949ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a71553ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "causal_template = json.load(open(\"input/2_causal_interpretability.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d99946",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "\n",
    "    #model = \"ollama/codellama:13b\", #13b\n",
    "    model = \"ollama/gpt-oss:latest\", #20b\n",
    "    #model = \"ollama/devstral:latest\", #24b\n",
    "   # model = \"ollama/mistral-small:latest\", #22b\n",
    "    base_url = \"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3de4007",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67e19301",
   "metadata": {},
   "source": [
    "## Scenario A: Code generation text2code task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81750c8",
   "metadata": {},
   "source": [
    "### Step 1: Define the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce1aab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_a = causal_template['scenario_a']['step1']['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f02ffb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an expert in causal inference and counterfactual reasoning. Your job is to infer a causal graph from a set of variables and draft counterfactual explanations.'},\n",
       " {'role': 'user',\n",
       "  'content': '[CAUSAL TASK] Giving the following variable in the json file {FACTORS}, generate a causal graph that explains how they influence each other. Consider that those variables are influencing the generation of the function {FUNCTION}, therefore the code generation should be a separate node in the graph. The outcome must be in Graphviz DOT format.'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software developer had not considered the variable {NODE} during the generation of the {FUNCTION} function? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec6c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages_a[1]['content'] =' '.join([messages_a[0]['content'], messages_a[1]['content']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a84fa354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages_a[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a94ac9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function_name(response):\n",
    "    match = re.search(r\"def\\s+(\\w+)\\s*\\(\", response)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffcaeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_messages(message,function, json_factors,node, test):\n",
    "    #new_message= re.sub(r\"\\[.*?\\]\",\"\", message['content'])\n",
    "    new_message= message['content']\n",
    "    if function:\n",
    "        new_message = re.sub(r\"\\{FUNCTION\\}\",function, new_message)\n",
    "    if json_factors:\n",
    "        new_message = re.sub(r\"\\{FACTORS\\}\",json_factors, new_message)\n",
    "    if re.search(r\"\\{NODE\\}\", new_message):\n",
    "        new_message = re.sub(r\"\\{NODE\\}\",node, new_message)\n",
    "    if re.search(r\"\\{TEST\\}\", new_message):\n",
    "        new_message = re.sub(r\"\\{TEST\\}\",test, new_message)\n",
    "       \n",
    "    return new_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b9dc540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_name(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            if k == \"name\":\n",
    "                yield v\n",
    "            else:\n",
    "                yield from find_name(v)\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            yield from find_name(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15851135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_introspection_factors(introspection_chat_history):\n",
    "    factors = []\n",
    "    function = None\n",
    "    function_message = introspection_chat_history[3]['content']\n",
    "    function = extract_function_name(function_message)\n",
    "    factors_message = introspection_chat_history[5]['content']\n",
    "    cleaned = re.sub(r\"^```json\\s*|\\s*```$\", \"\", factors_message.strip(), flags=re.MULTILINE)\n",
    "\n",
    "    data = json.loads(cleaned)\n",
    "    factors=list(find_name(data))\n",
    "    return function, function_message, factors, factors_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55dca6a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "process_messages() missing 1 required positional argument: 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messagges:\n\u001b[32m     25\u001b[39m     last_message = copy.deepcopy(message)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m nodes:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(last_message)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(message, node)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(message, node):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     message[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mprocess_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactors_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(message[\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-----\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: process_messages() missing 1 required positional argument: 'test'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "chat_history = []\n",
    "last_response = ''\n",
    "messagges = copy.deepcopy(messages_a)\n",
    "input_file= 'ollama_gpt-oss:latest_step1_step2_sptep3_15.json'\n",
    "introspection_messaages =json.load(open(\"output/introspection/\"+input_file))['scenario_a']['step1']['messages']\n",
    "\n",
    "func, func_name, nodes, factors_message = process_introspection_factors(introspection_messaages)\n",
    "\n",
    "\n",
    "first_node = nodes.pop(0)\n",
    "last_message = {}\n",
    "def run(message, node):\n",
    "    \n",
    "    message['content'] = process_messages(message, func, factors_message, node)\n",
    "    print(message['content'])\n",
    "    print(\"-----\")\n",
    "    chat_history.append(message)\n",
    "    last_response = llm.call(messages=chat_history)\n",
    "    #print({'role':'assistant','content':last_response})\n",
    "    print(last_response)\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": last_response})\n",
    "\n",
    "for message in messagges:\n",
    "    last_message = copy.deepcopy(message)\n",
    "    run(message , first_node)\n",
    "\n",
    "while nodes:\n",
    "    print(last_message)\n",
    "    run(last_message, nodes.pop(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dbb1149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an expert in causal inference and counterfactual reasoning. Your job is to infer a causal graph from a set of variables and draft counterfactual explanations.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Sure thing! To get started, could you share the list of variables you’re interested in and the context (e.g., a study design, a real‑world system, or a hypothetical scenario)? Once I know the variables and any known relationships or constraints, I can:\\n\\n1. Sketch a directed acyclic graph (DAG) that represents the plausible causal structure.\\n2. Identify key paths, colliders, and potential confounders.\\n3. Draft counterfactual explanations for a few key interventions or events in that system.\\n\\nFeel free to provide any additional background (e.g., data sources, domain knowledge, or specific questions you want to answer) so I can tailor the DAG and explanations accordingly.'},\n",
       " {'role': 'user',\n",
       "  'content': '[CAUSAL TASK] Giving the following variable in the json file ```json\\n{\\n  \"A\": {\\n    \"name\": \"Type hint usage\",\\n    \"description\": \"Adding explicit type hints (Sequence[float], List[float]) clarifies expected inputs and outputs, improving static analysis and developer experience.\",\\n    \"attribution_weight\": 0.20\\n  },\\n  \"B\": {\\n    \"name\": \"Input validation\",\\n    \"description\": \"Checking that both vectors have exactly three elements ensures the function behaves correctly and prevents subtle bugs.\",\\n    \"attribution_weight\": 0.20\\n  },\\n  \"C\": {\\n    \"name\": \"Error handling\",\\n    \"description\": \"Raising a ValueError with a clear message helps callers quickly identify misuse of the API.\",\\n    \"attribution_weight\": 0.15\\n  },\\n  \"D\": {\\n    \"name\": \"Documentation string\",\\n    \"description\": \"A comprehensive docstring (including parameters, returns, and examples) aids readability, IDE assistance, and future maintenance.\",\\n    \"attribution_weight\": 0.15\\n  },\\n  \"E\": {\\n    \"name\": \"Function name\",\\n    \"description\": \"Choosing a concise, descriptive name (cross_product) makes the intent immediately obvious.\",\\n    \"attribution_weight\": 0.10\\n  },\\n  \"F\": {\\n    \"name\": \"Return type\",\\n    \"description\": \"Returning a concrete List[float] rather than a generic Sequence improves usability for callers expecting a mutable sequence.\",\\n    \"attribution_weight\": 0.10\\n  },\\n  \"G\": {\\n    \"name\": \"Code readability\",\\n    \"description\": \"Using clear variable names (a1, a2, a3, etc.) and straightforward arithmetic enhances understandability and maintainability.\",\\n    \"attribution_weight\": 0.10\\n  }\\n}\\n```, generate a causal graph that explains how they influence each other. Consider that those variables are influencing the generation of the function cross_product, therefore the code generation should be a separate node in the graph. The outcome must be in Graphviz DOT format.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```dot\\ndigraph CausalGraph {\\n    // Node style\\n    node [shape=box, style=filled, fillcolor=\"#f0f0f0\", fontname=\"Helvetica\"];\\n\\n    // Variables (nodes)\\n    A  [label=\"A\\\\nType hint usage\\\\n(0.20)\"];\\n    B  [label=\"B\\\\nInput validation\\\\n(0.20)\"];\\n    C  [label=\"C\\\\nError handling\\\\n(0.15)\"];\\n    D  [label=\"D\\\\nDocumentation string\\\\n(0.15)\"];\\n    E  [label=\"E\\\\nFunction name\\\\n(0.10)\"];\\n    F  [label=\"F\\\\nReturn type\\\\n(0.10)\"];\\n    G  [label=\"G\\\\nCode readability\\\\n(0.10)\"];\\n    CodeGeneration [label=\"Code generation\\\\n(cross_product)\"];\\n\\n    // Causal edges\\n    // Type hint usage influences several aspects\\n    A -> B;          // Type hints encourage input validation\\n    A -> D;          // Clear types help in writing better docs\\n    A -> G;          // Types promote readability\\n\\n    // Input validation cascades to error handling and readability\\n    B -> C;          // Validations lead to explicit error messages\\n    B -> G;          // Validations simplify code logic\\n\\n    // Error handling improves readability\\n    C -> G;\\n\\n    // Documentation and naming both aid readability\\n    D -> G;\\n    E -> G;\\n\\n    // Return type is part of the overall design, affecting readability\\n    F -> G;\\n\\n    // Core design decisions feed into the final code\\n    D -> CodeGeneration;\\n    E -> CodeGeneration;\\n    G -> CodeGeneration;\\n}\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software developer had not considered the variable Type hint usage during the generation of the cross_product function? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```json\\n{\\n  \"Missing_Type_Hints_Causes_Runtime_Errors\": {\\n    \"description\": \"Without explicit type hints the developer is less likely to perform input validation, resulting in more runtime errors when unexpected values are passed to the function.\",\\n    \"rationale\": [\\n      \"The causal graph shows that Type hint usage (A) directly influences Input validation (B).\",\\n      \"When A is absent, the probability of B occurring drops, so the developer is more likely to skip or weaken input validation.\",\\n      \"Input validation (B) is a prerequisite for Error handling (C). Without B, C is weakened or omitted.\",\\n      \"Missing validation allows invalid inputs to propagate to the computation, which in turn triggers runtime errors or incorrect results.\",\\n      \"Thus the absence of A cascades through B (and C) to increase the likelihood of runtime errors.\"\\n    ],\\n    \"probability\": 0.8\\n  },\\n  \"Documentation_Quality_Diminishes\": {\\n    \"description\": \"Type hints guide the creation of a comprehensive documentation string; without them the docstring may be incomplete or misleading, confusing future developers.\",\\n    \"rationale\": [\\n      \"The graph indicates that A influences Documentation string (D) directly.\",\\n      \"If A is not considered, the developer has less structural guidance for D, reducing its completeness.\",\\n      \"A weaker D leads to missing parameter descriptions, return type explanations, and usage examples.\",\\n      \"Incomplete documentation makes it harder for other developers to understand or correctly use the function, especially when coupled with ambiguous input types.\"\\n    ],\\n    \"probability\": 0.6\\n  },\\n  \"Code_Readability_Decreases\": {\\n    \"description\": \"Without type hints, variable naming and expression clarity suffer, resulting in lower overall code readability and higher maintenance effort.\",\\n    \"rationale\": [\\n      \"The causal model links A to Code readability (G) as a direct influence.\",\\n      \"Absent A reduces G, but readability also receives input from B, C, D, E, and F. With weaker B and D, the combined effect on G is significant.\",\\n      \"Consequently, the final Code generation step receives a lower-quality G input, producing code that is harder to read and maintain.\",\\n      \"Low readability can delay debugging and future enhancements, increasing the risk of hidden bugs.\"\\n    ],\\n    \"probability\": 0.5\\n  }\\n}\\n```\\n'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software developer had not considered the variable Input validation during the generation of the cross_product function? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```json\\n{\\n  \"Missing_Input_Validation_Leads_To_Runtime_Errors\": {\\n    \"description\": \"If the developer does not implement input validation (B), the function is more likely to receive malformed vectors, causing arithmetic operations to raise runtime exceptions or produce incorrect results.\",\\n    \"rationale\": [\\n      \"In the causal graph, B is a direct parent of C (Error handling).\",\\n      \"When B is absent, the probability that C is implemented correctly drops because no explicit checks are performed to trigger error handling.\",\\n      \"Without validation, unexpected values (e.g., wrong length, non‑numeric elements) propagate into the cross‑product calculation, leading to runtime errors such as IndexError, TypeError, or silent incorrect outputs.\",\\n      \"Thus the absence of B cascades through C and ultimately increases the likelihood of runtime failures.\"\\n    ],\\n    \"probability\": 0.8\\n  },\\n  \"Reduced_Clarity_of_Error_Messages\": {\\n    \"description\": \"The lack of input validation diminishes the quality and presence of error handling, resulting in vague or missing error messages that make debugging more difficult.\",\\n    \"rationale\": [\\n      \"B directly influences C; without B, the developer is less inclined to raise a ValueError with a clear explanation.\",\\n      \"C is a direct parent of G (Readability). A weaker or missing C leads to lower readability because error messages are either absent or cryptic.\",\\n      \"This makes it harder for callers to understand why a failure occurred, increasing maintenance overhead.\"\\n    ],\\n    \"probability\": 0.6\\n  },\\n  \"Lowered_Code_Readability_and_Maintainability\": {\\n    \"description\": \"Without input validation, the code must include additional guard logic or rely on unchecked arithmetic, reducing overall readability and making future modifications riskier.\",\\n    \"rationale\": [\\n      \"B is a direct parent of G; omitting B causes a drop in G (readability) because the code contains less structured checks.\",\\n      \"G also receives inputs from C, D, E, and F; with B missing, the combined effect on G is substantial, leading to a less clean implementation.\",\\n      \"Lower G value propagates to CodeGeneration, producing a function that is harder to understand, test, and extend, thereby raising the chance of hidden bugs during future edits.\"\\n    ],\\n    \"probability\": 0.5\\n  }\\n}\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software developer had not considered the variable Input validation during the generation of the cross_product function? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```json\\n{\\n  \"Runtime_Errors_from_Missing_Validation\": {\\n    \"description\": \"If the developer omits input validation, malformed vectors (wrong length or non‑numeric elements) can reach the arithmetic core, causing runtime errors such as `IndexError`, `TypeError`, or silent mis‑computations.\",\\n    \"rationale\": [\\n      \"The causal graph shows that B (Input validation) directly influences C (Error handling).\",\\n      \"Without B, the probability that C is implemented or triggers correctly diminishes.\",\\n      \"C normally captures and reports invalid inputs; lacking it allows those inputs to propagate into the cross‑product calculation.\",\\n      \"The arithmetic on improperly shaped or typed data then raises runtime exceptions or produces incorrect results.\",\\n      \"Thus, the absence of B increases the likelihood of runtime failures.\"\\n    ],\\n    \"probability\": 0.8\\n  },\\n  \"Vague_or_Missing_Error_Messages\": {\\n    \"description\": \"Without input validation, the function may not raise clear `ValueError`s, leading to ambiguous or missing error messages that make debugging harder.\",\\n    \"rationale\": [\\n      \"B → C: missing validation reduces the likelihood that C (Error handling) is present or meaningful.\",\\n      \"C is a direct parent of G (Readability); a weak or absent C lowers readability.\",\\n      \"Lower readability makes it difficult for callers to identify the cause of a failure, increasing maintenance overhead.\",\\n      \"Therefore, omitting B often results in vague or missing error information.\"\\n    ],\\n    \"probability\": 0.6\\n  },\\n  \"Decreased_Code_Readability_and_Maintainability\": {\\n    \"description\": \"Skipping input validation makes the implementation more cluttered or rely on unchecked arithmetic, thereby reducing overall code readability and making future edits riskier.\",\\n    \"rationale\": [\\n      \"B is a parent of G; without B, G (Readability) receives less structured guard logic.\",\\n      \"G also aggregates inputs from C, D, E, and F; the absence of B weakens the combined contribution to G.\",\\n      \"Lower G propagates to CodeGeneration, yielding code that is harder to understand, test, and extend.\",\\n      \"Consequently, maintenance becomes more error‑prone.\"\\n    ],\\n    \"probability\": 0.5\\n  }\\n}\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software developer had not considered the variable Input validation during the generation of the cross_product function? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```json\\n{\\n  \"Runtime_Errors_From_Unvalidated_Inputs\": {\\n    \"description\": \"Skipping input validation allows malformed vectors to reach the arithmetic core, which often triggers runtime exceptions (e.g., `IndexError`, `TypeError`) or silent mis‑calculations.\",\\n    \"rationale\": [\\n      \"The DAG shows B (Input validation) is a direct parent of C (Error handling).\",\\n      \"When B is omitted, the likelihood that C is correctly implemented or invoked diminishes.\",\\n      \"Without C, the function lacks checks for vector length and element type, so invalid data propagates into the cross‑product computation.\",\\n      \"The arithmetic on improperly shaped or typed data leads to runtime failures or incorrect results.\",\\n      \"Thus, the absence of B raises the probability of encountering runtime errors.\"\\n    ],\\n    \"probability\": 0.75\\n  },\\n  \"Vague_or_Missing_Error_Messages\": {\\n    \"description\": \"Without input validation, the function rarely raises clear `ValueError`s, resulting in ambiguous or missing error messages that make debugging more difficult.\",\\n    \"rationale\": [\\n      \"B → C: omitting validation reduces the chance that C (Error handling) is present or that it raises informative exceptions.\",\\n      \"C is a parent of G (Readability); a weak or absent C lowers overall readability of the code.\",\\n      \"Lower readability obscures the source of failures, making it harder for callers to diagnose and fix problems.\",\\n      \"Consequently, the probability of encountering unclear or missing error messages increases.\"\\n    ],\\n    \"probability\": 0.55\\n  },\\n  \"Decreased_Code_Readability_and_Maintainability\": {\\n    \"description\": \"Skipping input validation results in less structured guard logic, which lowers overall code readability and makes future edits more error‑prone.\",\\n    \"rationale\": [\\n      \"B is a parent of G; when B is absent, G (Readability) receives fewer explicit checks and clearer flow control.\",\\n      \"G also aggregates inputs from C, D, E, and F; missing B weakens the combined contribution to G.\",\\n      \"Lower G propagates to CodeGeneration, producing a function that is harder to understand, test, and extend.\",\\n      \"This reduced readability elevates the risk of hidden bugs during maintenance.\"\\n    ],\\n    \"probability\": 0.45\\n  }\\n}\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software developer had not considered the variable Input validation during the generation of the cross_product function? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```json\\n{\\n  \"Runtime_Errors_Without_Validation\": {\\n    \"description\": \"If the developer omits input validation, malformed vectors (wrong length or non‑numeric elements) can reach the arithmetic core, causing runtime exceptions such as `IndexError`, `TypeError`, or silent mis‑computations.\",\\n    \"rationale\": [\\n      \"The DAG shows that B (Input validation) is a direct parent of C (Error handling).\",\\n      \"When B is omitted, the probability that C is correctly implemented or invoked decreases.\",\\n      \"C normally captures and reports invalid inputs; lacking it allows those inputs to propagate into the cross‑product calculation.\",\\n      \"The arithmetic on improperly shaped or typed data then raises runtime errors or produces incorrect results.\",\\n      \"Thus the absence of B increases the likelihood of encountering runtime failures.\"\\n    ],\\n    \"probability\": 0.8\\n  },\\n  \"Vague_or_Missing_Error_Messages\": {\\n    \"description\": \"Without input validation, the function rarely raises clear `ValueError`s, resulting in ambiguous or missing error messages that make debugging more difficult.\",\\n    \"rationale\": [\\n      \"B → C: omitting validation reduces the chance that C (Error handling) is present or that it raises informative exceptions.\",\\n      \"C is a parent of G (Readability); a weak or absent C lowers overall readability of the code.\",\\n      \"Lower readability obscures the source of failures, making it harder for callers to diagnose and fix problems.\",\\n      \"Consequently, the probability of encountering unclear or missing error messages increases.\"\\n    ],\\n    \"probability\": 0.55\\n  },\\n  \"Decreased_Code_Readability_and_Maintainability\": {\\n    \"description\": \"Skipping input validation results in less structured guard logic, which lowers overall code readability and makes future edits more error‑prone.\",\\n    \"rationale\": [\\n      \"B is a parent of G; when B is absent, G (Readability) receives fewer explicit checks and clearer flow control.\",\\n      \"G also aggregates inputs from C, D, E, and F; missing B weakens the combined contribution to G.\",\\n      \"Lower G propagates to CodeGeneration, producing a function that is harder to understand, test, and extend.\",\\n      \"This reduced readability elevates the risk of hidden bugs during maintenance.\"\\n    ],\\n    \"probability\": 0.45\\n  }\\n}\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software developer had not considered the variable Input validation during the generation of the cross_product function? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```json\\n{\\n  \"Runtime_Failures_Due_to_Unvalidated_Inputs\": {\\n    \"description\": \"Without input validation the function can receive malformed vectors, causing runtime errors (e.g., IndexError, TypeError) or silent mis‑calculations during the cross‑product computation.\",\\n    \"rationale\": [\\n      \"B (Input validation) is a direct parent of C (Error handling) in the causal graph.\",\\n      \"When B is omitted, the probability that C is implemented or invoked correctly drops.\",\\n      \"C normally intercepts invalid inputs and raises a clear exception; without C, bad data propagates into the arithmetic core.\",\\n      \"The arithmetic on incorrectly shaped or typed vectors triggers runtime failures or incorrect outputs.\",\\n      \"Thus the absence of B directly increases the likelihood of runtime errors.\"\\n    ],\\n    \"probability\": 0.8\\n  },\\n  \"Vague_or_Missing_Error_Messages\": {\\n    \"description\": \"Skipping input validation leads to fewer or less informative ValueError messages, making debugging more difficult.\",\\n    \"rationale\": [\\n      \"B → C: when validation is missing, the likelihood that C (Error handling) raises a meaningful exception decreases.\",\\n      \"C is a parent of G (Readability); a weak or absent C reduces overall readability.\",\\n      \"Lower readability obscures the source of failures, so callers receive ambiguous or missing error information.\",\\n      \"Consequently, the probability of encountering vague or missing error messages rises.\"\\n    ],\\n    \"probability\": 0.6\\n  },\\n  \"Lowered_Code_Readability_and_Maintainability\": {\\n    \"description\": \"Omitting input validation results in less structured guard logic, which diminishes code readability and makes future modifications more error‑prone.\",\\n    \"rationale\": [\\n      \"B is a parent of G; without B, G (Readability) receives fewer explicit checks and clearer flow control.\",\\n      \"G aggregates contributions from C, D, E, and F; missing B weakens the combined input to G.\",\\n      \"A lower G value propagates to CodeGeneration, yielding a function that is harder to understand, test, and extend.\",\\n      \"Reduced readability increases the risk of hidden bugs during maintenance.\"\\n    ],\\n    \"probability\": 0.5\\n  }\\n}\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software developer had not considered the variable Input validation during the generation of the cross_product function? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```json\\n{\\n  \"Runtime_Failures_Due_to_Unvalidated_Inputs\": {\\n    \"description\": \"If input validation (B) is omitted, the function may receive malformed vectors (wrong length, non‑numeric elements), which will cause runtime errors such as `IndexError`, `TypeError`, or silent mis‑calculations during the cross‑product computation.\",\\n    \"rationale\": [\\n      \"In the DAG, B is a direct parent of C (Error handling).\",\\n      \"Without B, the probability that C is correctly implemented or invoked decreases.\",\\n      \"C normally intercepts invalid inputs and raises a clear exception; without it, bad data propagates into the arithmetic core.\",\\n      \"The arithmetic on incorrectly shaped or typed vectors triggers runtime failures or incorrect outputs.\",\\n      \"Therefore, the absence of B directly increases the likelihood of runtime errors.\"\\n    ],\\n    \"probability\": 0.8\\n  },\\n  \"Vague_or_Missing_Error_Messages\": {\\n    \"description\": \"Skipping input validation leads to fewer or less informative `ValueError` messages, making debugging and troubleshooting more difficult.\",\\n    \"rationale\": [\\n      \"B → C: when validation is missing, the chance that C (Error handling) raises a meaningful exception decreases.\",\\n      \"C is a parent of G (Readability); a weak or absent C reduces overall code readability.\",\\n      \"Lower readability obscures the source of failures, so callers receive ambiguous or missing error information.\",\\n      \"Consequently, the probability of encountering vague or missing error messages rises.\"\\n    ],\\n    \"probability\": 0.6\\n  },\\n  \"Decreased_Code_Readability_and_Maintainability\": {\\n    \"description\": \"Omitting input validation results in less structured guard logic, which diminishes overall code readability and makes future modifications more error‑prone.\",\\n    \"rationale\": [\\n      \"B is a parent of G; without B, G (Readability) receives fewer explicit checks and clearer flow control.\",\\n      \"G aggregates contributions from C, D, E, and F; missing B weakens the combined input to G.\",\\n      \"A lower G value propagates to CodeGeneration, yielding a function that is harder to understand, test, and extend.\",\\n      \"Reduced readability increases the risk of hidden bugs during maintenance.\"\\n    ],\\n    \"probability\": 0.45\\n  }\\n}\\n```\\n'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94ca2128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an expert in causal inference and counterfactual reasoning. Your job is to infer a causal graph from a set of variables and draft counterfactual explanations.'},\n",
       " {'role': 'user',\n",
       "  'content': '[CAUSAL TASK] Giving the following variable in the json file {FACTORS}, generate a causal graph that explains how they influence each other. Consider that those variables are influencing the generation of the function {FUNCTION}, therefore the code generation should be a separate node in the graph. The outcome must be in Graphviz DOT format.'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software developer had not considered the variable {NODE} during the generation of the {FUNCTION} function? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "125e68b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cell run number is: 14\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "# Get the current execution count\n",
    "exec_count = get_ipython().execution_count\n",
    "print(\"This cell run number is:\", exec_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1598a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output ={\n",
    "    'scenario_a': {\n",
    "        'step1': {'messages': chat_history}\n",
    "    }\n",
    "}\n",
    "file = \"output/interpretability/\"+ llm.model.replace(\"/\",\"_\")+\"_step1_\"+ str(exec_count) +\".json\"\n",
    "with open(file, \"w\") as f:\n",
    "    json.dump(output, f, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f04d3a5",
   "metadata": {},
   "source": [
    "# Scenario A - interpretability - step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18086d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_2 = causal_template['scenario_a']['step2']['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2eb2bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an expert in causal inference and counterfactual reasoning. Your job is to infer a causal graph from a set of variables and draft counterfactual explanations.'},\n",
       " {'role': 'user',\n",
       "  'content': '[CAUSAL TASK] Giving the following variables in the json file {FACTORS}, generate a causal graph that explains how they influence each other. Consider that those variables are influencing the generation of the test case {TEST}, therefore the code generation should be a separate node in the graph. The outcome must be in Graphviz DOT format.'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable {NODE} during the generation of the {TEST} test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f779f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in causal inference and counterfactual reasoning. Your job is to infer a causal graph from a set of variables and draft counterfactual explanations.\n",
      "-----\n",
      "Got it! To get started on building a causal graph and drafting counterfactual explanations, I’ll need a bit more information from you:\n",
      "\n",
      "1. **Variables**  \n",
      "   - List all the variables you’re interested in (e.g., `X`, `Y`, `Z`, etc.).  \n",
      "   - If possible, give short definitions or notes on what each variable represents (e.g., “`Age` – age of the participant”, “`Treatment` – whether a subject received drug A” …).\n",
      "\n",
      "2. **Domain Knowledge / Prior Knowledge**  \n",
      "   - Any known causal relationships (e.g., “Age → Treatment” or “Gender → Outcome”).  \n",
      "   - Any constraints (e.g., “`Outcome` cannot cause `Treatment` because treatment is administered first”).  \n",
      "   - Any known confounders or mediators.\n",
      "\n",
      "3. **Data / Observational Facts**  \n",
      "   - If you have a dataset, a summary (means, correlations) or any patterns you’ve noticed.  \n",
      "   - If you only have an observational description, any qualitative trends (e.g., “higher `Income` tends to lead to higher `HealthScore`”).\n",
      "\n",
      "4. **Research Question / Counterfactual Scenario**  \n",
      "   - What specific counterfactuals are you interested in?  \n",
      "   - Example: “If a patient had not received the treatment, would their outcome have been different?” or “What would happen to `Outcome` if we increased `Exposure` by one unit?”\n",
      "\n",
      "Once I have that information, I can:\n",
      "\n",
      "- Sketch a directed acyclic graph (DAG) that captures the plausible causal structure.  \n",
      "- Identify confounders, mediators, colliders, and any necessary adjustment sets.  \n",
      "- Derive counterfactual queries and provide narrative explanations (e.g., “Because the treatment has a direct effect on the outcome and is influenced by confounder `X`, adjusting for `X` allows us to estimate the causal effect…”).  \n",
      "\n",
      "Feel free to paste the variable list and any notes, and we’ll dive in!\n",
      "[CAUSAL TASK] Giving the following variables in the json file {\n",
      "  \"A\": {\n",
      "    \"name\": \"Function Complexity\",\n",
      "    \"description\": \"The number of logical branches, nested conditions, and overall algorithmic complexity of the function.\",\n",
      "    \"weight\": 0.25\n",
      "  },\n",
      "  \"B\": {\n",
      "    \"name\": \"Input Validation\",\n",
      "    \"description\": \"Whether the function checks the validity of its inputs (e.g., length, type, value ranges).\",\n",
      "    \"weight\": 0.20\n",
      "  },\n",
      "  \"C\": {\n",
      "    \"name\": \"Error Handling\",\n",
      "    \"description\": \"The presence and robustness of error handling logic, such as raising exceptions for invalid cases.\",\n",
      "    \"weight\": 0.15\n",
      "  },\n",
      "  \"D\": {\n",
      "    \"name\": \"Dependencies\",\n",
      "    \"description\": \"External modules, services, or global state that the function relies on, influencing isolation in tests.\",\n",
      "    \"weight\": 0.10\n",
      "  },\n",
      "  \"E\": {\n",
      "    \"name\": \"Documentation/Contract\",\n",
      "    \"description\": \"Clarity of the function's docstring, parameter and return type contracts, and adherence to specifications.\",\n",
      "    \"weight\": 0.15\n",
      "  },\n",
      "  \"F\": {\n",
      "    \"name\": \"Edge Cases\",\n",
      "    \"description\": \"The likelihood of encountering unusual or boundary inputs that could reveal hidden bugs.\",\n",
      "    \"weight\": 0.15\n",
      "  }\n",
      "}, generate a causal graph that explains how they influence each other. Consider that those variables are influencing the generation of the test case import unittest\n",
      "from typing import Sequence, List\n",
      "\n",
      "def cross_product(a: Sequence[float], b: Sequence[float]) -> List[float]:\n",
      "    \"\"\"\n",
      "    Compute the cross product of two 3‑D vectors.\n",
      "    \"\"\"\n",
      "    if len(a) != 3 or len(b) != 3:\n",
      "        raise ValueError(\"Both vectors must be of length 3.\")\n",
      "    a1, a2, a3 = a\n",
      "    b1, b2, b3 = b\n",
      "    return [\n",
      "        a2 * b3 - a3 * b2,\n",
      "        a3 * b1 - a1 * b3,\n",
      "        a1 * b2 - a2 * b1,\n",
      "    ]\n",
      "\n",
      "class TestCrossProduct(unittest.TestCase):\n",
      "    def test_basic_cross_product(self):\n",
      "        a = [1, 2, 3]\n",
      "        b = [4, 5, 6]\n",
      "        expected = [-3, 6, -3]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_with_zero_vector(self):\n",
      "        a = [0, 0, 0]\n",
      "        b = [1, 2, 3]\n",
      "        expected = [0, 0, 0]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_orthogonal_vectors(self):\n",
      "        a = [1, 0, 0]\n",
      "        b = [0, 1, 0]\n",
      "        expected = [0, 0, 1]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_parallel_vectors(self):\n",
      "        a = [2, 4, 6]\n",
      "        b = [1, 2, 3]\n",
      "        expected = [0, 0, 0]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_negative_vector(self):\n",
      "        a = [-1, 0, 0]\n",
      "        b = [0, 1, 0]\n",
      "        expected = [0, 0, -1]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_return_type(self):\n",
      "        a = (1, 2, 3)\n",
      "        b = [4, 5, 6]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertIsInstance(result, list)\n",
      "        self.assertEqual(len(result), 3)\n",
      "\n",
      "    def test_cross_product_invalid_length(self):\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([1, 2], [3, 4, 5])\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([1, 2, 3], [4, 5])\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([], [])\n",
      "\n",
      "    def test_cross_product_non_sequence_inputs(self):\n",
      "        with self.assertRaises(TypeError):\n",
      "            cross_product([1, 2, 3], None)\n",
      "        with self.assertRaises(TypeError):\n",
      "            cross_product(None, [1, 2, 3])\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    unittest.main(), therefore the code generation should be a separate node in the graph. The outcome must be in Graphviz DOT format.\n",
      "-----\n",
      "**Causal graph (Graphviz DOT)**  \n",
      "\n",
      "```dot\n",
      "digraph causal_graph {\n",
      "    // Overall layout\n",
      "    rankdir=LR;\n",
      "    node [shape=box, style=rounded, fontsize=10];\n",
      "\n",
      "    // ---- Variables ----------------------------------------------------\n",
      "    A [label=\"Function\\nComplexity\\n(A)\"];\n",
      "    B [label=\"Input\\nValidation\\n(B)\"];\n",
      "    C [label=\"Error\\nHandling\\n(C)\"];\n",
      "    D [label=\"Dependencies\\n(D)\"];\n",
      "    E [label=\"Documentation/Contract\\n(E)\"];\n",
      "    F [label=\"Edge\\nCases\\n(F)\"];\n",
      "\n",
      "    // ---- Process nodes -------------------------------------------------\n",
      "    CodeGen      [label=\"Code\\nGeneration\"];\n",
      "    TestCaseGen  [label=\"Test Case\\nGeneration\"];\n",
      "\n",
      "    // ---- Direct causal links -------------------------------------------\n",
      "    // Complexity drives the need for more validation, error handling,\n",
      "    // and exposes more edge cases\n",
      "    A -> B;\n",
      "    A -> C;\n",
      "    A -> F;\n",
      "    A -> CodeGen;\n",
      "\n",
      "    // Validation supports better error handling and clear documentation\n",
      "    B -> C;\n",
      "    B -> E;\n",
      "    B -> CodeGen;\n",
      "\n",
      "    // Error handling enriches documentation contracts\n",
      "    C -> E;\n",
      "    C -> CodeGen;\n",
      "\n",
      "    // External dependencies increase validation needs,\n",
      "    // error handling complexity, and expose new edge cases\n",
      "    D -> B;\n",
      "    D -> C;\n",
      "    D -> F;\n",
      "    D -> CodeGen;\n",
      "\n",
      "    // Documentation helps anticipate edge cases\n",
      "    E -> F;\n",
      "    E -> CodeGen;\n",
      "\n",
      "    // Edge cases influence how the function is coded\n",
      "    F -> CodeGen;\n",
      "\n",
      "    // Code generation is the source of the test‑case code\n",
      "    CodeGen -> TestCaseGen;\n",
      "}\n",
      "```\n",
      "\n",
      "### How the graph reads\n",
      "\n",
      "- **A → B, C, F, CodeGen** – A more complex function naturally requires more input validation, better error handling, and has a higher likelihood of edge cases, all of which shape the generated code.\n",
      "- **B → C, E, CodeGen** – Thorough input validation drives robust error handling and is reflected in the function’s documentation. Both aspects directly influence the code that is produced.\n",
      "- **C → E, CodeGen** – Good error‑handling practices improve the clarity of contracts and documentation, and they are also embedded in the code.\n",
      "- **D → B, C, F, CodeGen** – Relying on external modules or services forces stricter validation, richer error handling, and can introduce additional edge cases; all of this alters the generated code.\n",
      "- **E → F, CodeGen** – Clear documentation can expose hidden edge cases and guides the generation of corresponding code.\n",
      "- **F → CodeGen** – Anticipated edge cases dictate defensive coding patterns.\n",
      "- **CodeGen → TestCaseGen** – The code produced in the *Code Generation* node is the sole source of the *Test Case Generation* node. All other variables affect test‑case creation **only** through their influence on the generated code.\n",
      "\n",
      "Feel free to copy‑paste the DOT snippet into any Graphviz renderer (e.g., `dot -Tpng`) to visualise the causal structure.\n",
      "[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable Function Complexity during the generation of the import unittest\n",
      "from typing import Sequence, List\n",
      "\n",
      "def cross_product(a: Sequence[float], b: Sequence[float]) -> List[float]:\n",
      "    \"\"\"\n",
      "    Compute the cross product of two 3‑D vectors.\n",
      "    \"\"\"\n",
      "    if len(a) != 3 or len(b) != 3:\n",
      "        raise ValueError(\"Both vectors must be of length 3.\")\n",
      "    a1, a2, a3 = a\n",
      "    b1, b2, b3 = b\n",
      "    return [\n",
      "        a2 * b3 - a3 * b2,\n",
      "        a3 * b1 - a1 * b3,\n",
      "        a1 * b2 - a2 * b1,\n",
      "    ]\n",
      "\n",
      "class TestCrossProduct(unittest.TestCase):\n",
      "    def test_basic_cross_product(self):\n",
      "        a = [1, 2, 3]\n",
      "        b = [4, 5, 6]\n",
      "        expected = [-3, 6, -3]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_with_zero_vector(self):\n",
      "        a = [0, 0, 0]\n",
      "        b = [1, 2, 3]\n",
      "        expected = [0, 0, 0]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_orthogonal_vectors(self):\n",
      "        a = [1, 0, 0]\n",
      "        b = [0, 1, 0]\n",
      "        expected = [0, 0, 1]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_parallel_vectors(self):\n",
      "        a = [2, 4, 6]\n",
      "        b = [1, 2, 3]\n",
      "        expected = [0, 0, 0]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_negative_vector(self):\n",
      "        a = [-1, 0, 0]\n",
      "        b = [0, 1, 0]\n",
      "        expected = [0, 0, -1]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_return_type(self):\n",
      "        a = (1, 2, 3)\n",
      "        b = [4, 5, 6]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertIsInstance(result, list)\n",
      "        self.assertEqual(len(result), 3)\n",
      "\n",
      "    def test_cross_product_invalid_length(self):\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([1, 2], [3, 4, 5])\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([1, 2, 3], [4, 5])\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([], [])\n",
      "\n",
      "    def test_cross_product_non_sequence_inputs(self):\n",
      "        with self.assertRaises(TypeError):\n",
      "            cross_product([1, 2, 3], None)\n",
      "        with self.assertRaises(TypeError):\n",
      "            cross_product(None, [1, 2, 3])\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    unittest.main() test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.\n",
      "-----\n",
      "```json\n",
      "{\n",
      "  \"Reduced_Edge_Case_Coverage\": {\n",
      "    \"description\": \"The test suite would miss several edge‑case scenarios such as zero vectors, parallel vectors, and negative components, because the tester did not account for the function’s complexity when generating tests.\",\n",
      "    \"rationale\": [\n",
      "      \"In the causal graph, Function Complexity (A) influences Edge Cases (F).\",\n",
      "      \"Edge Cases (F) directly affect Code Generation (CodeGen) and, consequently, Test Case Generation (TestCaseGen).\",\n",
      "      \"By ignoring A, the tester would not trigger the causal path A → F → CodeGen → TestCaseGen.\",\n",
      "      \"This omission results in a smaller set of generated tests, leading to uncovered edge‑case behaviors.\"\n",
      "    ],\n",
      "    \"probability\": 0.70\n",
      "  },\n",
      "  \"Missing_Input_Validation_Tests\": {\n",
      "    \"description\": \"The test suite would fail to include tests that validate input length and type, such as checks for non‑3‑D vectors or non‑sequence arguments, because the tester overlooked the complexity factor that normally drives thorough validation.\",\n",
      "    \"rationale\": [\n",
      "      \"Function Complexity (A) causes Input Validation (B) in the graph.\",\n",
      "      \"Input Validation (B) influences Error Handling (C) and Code Generation (CodeGen).\",\n",
      "      \"Code Generation (CodeGen) produces Test Case Generation (TestCaseGen).\",\n",
      "      \"If A is ignored, B receives no activation, so the causal chain A → B → CodeGen → TestCaseGen is cut.\",\n",
      "      \"Result: tests for invalid lengths and type errors are absent.\"\n",
      "    ],\n",
      "    \"probability\": 0.50\n",
      "  },\n",
      "  \"Incomplete_Contract_Compliance_Tests\": {\n",
      "    \"description\": \"The test suite would not fully verify that the function adheres to its documented contract (e.g., returning a list of length three), leading to potential contract violations going unnoticed.\",\n",
      "    \"rationale\": [\n",
      "      \"Documentation/Contract (E) is downstream of both Function Complexity (A) and Input Validation (B).\",\n",
      "      \"E feeds into Code Generation (CodeGen) and ultimately Test Case Generation (TestCaseGen).\",\n",
      "      \"Ignoring A removes its influence on E, reducing the likelihood that contract‑based tests (like checking return type and length) are created.\",\n",
      "      \"Thus, the resulting test suite lacks sufficient coverage of the documented contract.\"\n",
      "    ],\n",
      "    \"probability\": 0.40\n",
      "  }\n",
      "}\n",
      "```\n",
      "{'role': 'user', 'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable {NODE} during the generation of the {TEST} test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'}\n",
      "[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable Input Validation during the generation of the import unittest\n",
      "from typing import Sequence, List\n",
      "\n",
      "def cross_product(a: Sequence[float], b: Sequence[float]) -> List[float]:\n",
      "    \"\"\"\n",
      "    Compute the cross product of two 3‑D vectors.\n",
      "    \"\"\"\n",
      "    if len(a) != 3 or len(b) != 3:\n",
      "        raise ValueError(\"Both vectors must be of length 3.\")\n",
      "    a1, a2, a3 = a\n",
      "    b1, b2, b3 = b\n",
      "    return [\n",
      "        a2 * b3 - a3 * b2,\n",
      "        a3 * b1 - a1 * b3,\n",
      "        a1 * b2 - a2 * b1,\n",
      "    ]\n",
      "\n",
      "class TestCrossProduct(unittest.TestCase):\n",
      "    def test_basic_cross_product(self):\n",
      "        a = [1, 2, 3]\n",
      "        b = [4, 5, 6]\n",
      "        expected = [-3, 6, -3]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_with_zero_vector(self):\n",
      "        a = [0, 0, 0]\n",
      "        b = [1, 2, 3]\n",
      "        expected = [0, 0, 0]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_orthogonal_vectors(self):\n",
      "        a = [1, 0, 0]\n",
      "        b = [0, 1, 0]\n",
      "        expected = [0, 0, 1]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_parallel_vectors(self):\n",
      "        a = [2, 4, 6]\n",
      "        b = [1, 2, 3]\n",
      "        expected = [0, 0, 0]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_negative_vector(self):\n",
      "        a = [-1, 0, 0]\n",
      "        b = [0, 1, 0]\n",
      "        expected = [0, 0, -1]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_return_type(self):\n",
      "        a = (1, 2, 3)\n",
      "        b = [4, 5, 6]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertIsInstance(result, list)\n",
      "        self.assertEqual(len(result), 3)\n",
      "\n",
      "    def test_cross_product_invalid_length(self):\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([1, 2], [3, 4, 5])\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([1, 2, 3], [4, 5])\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([], [])\n",
      "\n",
      "    def test_cross_product_non_sequence_inputs(self):\n",
      "        with self.assertRaises(TypeError):\n",
      "            cross_product([1, 2, 3], None)\n",
      "        with self.assertRaises(TypeError):\n",
      "            cross_product(None, [1, 2, 3])\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    unittest.main() test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.\n",
      "-----\n",
      "```json\n",
      "{\n",
      "  \"Missing_Validation_Tests\": {\n",
      "    \"description\": \"The test suite would fail to include assertions for invalid input lengths or non‑sequence arguments, because the tester omitted the Input Validation (B) factor that normally drives such checks.\",\n",
      "    \"rationale\": [\n",
      "      \"In the causal graph, B is directly responsible for generating test cases that verify input length and type.\",\n",
      "      \"B influences Code Generation (CodeGen) through the edge B → CodeGen.\",\n",
      "      \"CodeGen is the sole source of Test Case Generation (TestCaseGen).\",\n",
      "      \"When B is not considered, the path B → CodeGen → TestCaseGen is severed.\",\n",
      "      \"Consequently, the test suite lacks cases that trigger ValueError/TypeError, leading to potential undiscovered failures when the function receives malformed inputs.\"\n",
      "    ],\n",
      "    \"probability\": 0.85\n",
      "  },\n",
      "  \"Uncovered_Error_Handling\": {\n",
      "    \"description\": \"The generated tests would not cover the error‑handling logic that raises ValueError for incorrect vector lengths, because the absence of B weakens the causal link to Error Handling (C).\",\n",
      "    \"rationale\": [\n",
      "      \"B → C is a direct edge; omitting B reduces the probability that C’s robust error paths are exercised.\",\n",
      "      \"C also feeds into CodeGen, but without B, the emphasis on guarding against invalid inputs is lowered.\",\n",
      "      \"Thus, even though the function contains a ValueError guard, the test suite may not exercise it, allowing runtime errors to slip through unnoticed.\",\n",
      "      \"This is especially problematic when the function is used in production code that might pass unexpected input shapes.\"\n",
      "    ],\n",
      "    \"probability\": 0.60\n",
      "  },\n",
      "  \"Reduced_Contract_Compliance_Tests\": {\n",
      "    \"description\": \"Tests that assert the function’s contract (e.g., return type is a list of length three) would be weaker or absent, because documentation/contract (E) depends on Input Validation (B).\",\n",
      "    \"rationale\": [\n",
      "      \"B → E is a causal link: thorough validation informs clearer documentation.\",\n",
      "      \"E influences CodeGen, which in turn creates TestCaseGen.\",\n",
      "      \"If B is omitted, the documentation component receives less signal, leading to fewer or less detailed contract‑based assertions in the test suite.\",\n",
      "      \"As a result, subtle contract violations—such as a mis‑typed return value—might remain undetected.\"\n",
      "    ],\n",
      "    \"probability\": 0.40\n",
      "  }\n",
      "}\n",
      "```\n",
      "{'role': 'user', 'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable {NODE} during the generation of the {TEST} test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'}\n",
      "[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable Error Handling during the generation of the import unittest\n",
      "from typing import Sequence, List\n",
      "\n",
      "def cross_product(a: Sequence[float], b: Sequence[float]) -> List[float]:\n",
      "    \"\"\"\n",
      "    Compute the cross product of two 3‑D vectors.\n",
      "    \"\"\"\n",
      "    if len(a) != 3 or len(b) != 3:\n",
      "        raise ValueError(\"Both vectors must be of length 3.\")\n",
      "    a1, a2, a3 = a\n",
      "    b1, b2, b3 = b\n",
      "    return [\n",
      "        a2 * b3 - a3 * b2,\n",
      "        a3 * b1 - a1 * b3,\n",
      "        a1 * b2 - a2 * b1,\n",
      "    ]\n",
      "\n",
      "class TestCrossProduct(unittest.TestCase):\n",
      "    def test_basic_cross_product(self):\n",
      "        a = [1, 2, 3]\n",
      "        b = [4, 5, 6]\n",
      "        expected = [-3, 6, -3]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_with_zero_vector(self):\n",
      "        a = [0, 0, 0]\n",
      "        b = [1, 2, 3]\n",
      "        expected = [0, 0, 0]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_orthogonal_vectors(self):\n",
      "        a = [1, 0, 0]\n",
      "        b = [0, 1, 0]\n",
      "        expected = [0, 0, 1]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_parallel_vectors(self):\n",
      "        a = [2, 4, 6]\n",
      "        b = [1, 2, 3]\n",
      "        expected = [0, 0, 0]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_negative_vector(self):\n",
      "        a = [-1, 0, 0]\n",
      "        b = [0, 1, 0]\n",
      "        expected = [0, 0, -1]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_return_type(self):\n",
      "        a = (1, 2, 3)\n",
      "        b = [4, 5, 6]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertIsInstance(result, list)\n",
      "        self.assertEqual(len(result), 3)\n",
      "\n",
      "    def test_cross_product_invalid_length(self):\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([1, 2], [3, 4, 5])\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([1, 2, 3], [4, 5])\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([], [])\n",
      "\n",
      "    def test_cross_product_non_sequence_inputs(self):\n",
      "        with self.assertRaises(TypeError):\n",
      "            cross_product([1, 2, 3], None)\n",
      "        with self.assertRaises(TypeError):\n",
      "            cross_product(None, [1, 2, 3])\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    unittest.main() test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.\n",
      "-----\n",
      "```json\n",
      "{\n",
      "  \"Missing_Error_Raising_Tests\": {\n",
      "    \"description\": \"The test suite would omit cases that trigger the function’s ValueError/TypeError logic because the tester did not take Error Handling (C) into account when generating tests.\",\n",
      "    \"rationale\": [\n",
      "      \"The causal graph contains a direct edge C → CodeGen, indicating that robust error‑handling code is translated into test‑case generation.\",\n",
      "      \"If C is ignored, the edge C → CodeGen is effectively cut, preventing the creation of tests that exercise the error‑raising branches.\",\n",
      "      \"Since the test cases that currently raise ValueError/TypeError depend on this path (e.g., `test_cross_product_invalid_length`, `test_cross_product_non_sequence_inputs`), their absence is highly probable.\",\n",
      "      \"Therefore, the likelihood that the test suite fails to include error‑raising tests is high.\"\n",
      "    ],\n",
      "    \"probability\": 0.80\n",
      "  },\n",
      "  \"Reduced_Edge_Case_Test_Coverage\": {\n",
      "    \"description\": \"Edge‑case scenarios that would normally trigger error handling (such as zero vectors or parallel vectors) would receive weaker or no coverage, because the absence of C weakens the link to CodeGen that would normally encode defensive checks.\",\n",
      "    \"rationale\": [\n",
      "      \"Edge Cases (F) influence CodeGen via F → CodeGen, but many edge‑case tests also rely on the presence of error‑handling logic to be meaningful.\",\n",
      "      \"C feeds into E and CodeGen; without C, the downstream path that enriches CodeGen with edge‑case logic is shortened.\",\n",
      "      \"Consequently, the test generator may produce fewer or less thorough edge‑case tests.\",\n",
      "      \"The probability of this outcome is moderate, as some edge‑case tests can still be produced through other paths (e.g., from Function Complexity or Dependencies).\"\n",
      "    ],\n",
      "    \"probability\": 0.60\n",
      "  },\n",
      "  \"Weak_Documentation_of_Error_Conditions\": {\n",
      "    \"description\": \"The function’s docstring/contract would omit references to the conditions that raise exceptions, because Documentation/Contract (E) is downstream of Error Handling (C).\",\n",
      "    \"rationale\": [\n",
      "      \"In the graph, C → E captures the idea that robust error handling informs clearer documentation.\",\n",
      "      \"If C is not considered, the signal that would normally enrich E is lost, leading to a docstring that may not mention ValueError or TypeError.\",\n",
      "      \"Because E influences CodeGen, the resulting test cases will also lack assertions that verify error conditions.\",\n",
      "      \"This outcome is less likely than missing error‑raising tests but still plausible.\"\n",
      "    ],\n",
      "    \"probability\": 0.40\n",
      "  }\n",
      "}\n",
      "```\n",
      "{'role': 'user', 'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable {NODE} during the generation of the {TEST} test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'}\n",
      "[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable Dependencies during the generation of the import unittest\n",
      "from typing import Sequence, List\n",
      "\n",
      "def cross_product(a: Sequence[float], b: Sequence[float]) -> List[float]:\n",
      "    \"\"\"\n",
      "    Compute the cross product of two 3‑D vectors.\n",
      "    \"\"\"\n",
      "    if len(a) != 3 or len(b) != 3:\n",
      "        raise ValueError(\"Both vectors must be of length 3.\")\n",
      "    a1, a2, a3 = a\n",
      "    b1, b2, b3 = b\n",
      "    return [\n",
      "        a2 * b3 - a3 * b2,\n",
      "        a3 * b1 - a1 * b3,\n",
      "        a1 * b2 - a2 * b1,\n",
      "    ]\n",
      "\n",
      "class TestCrossProduct(unittest.TestCase):\n",
      "    def test_basic_cross_product(self):\n",
      "        a = [1, 2, 3]\n",
      "        b = [4, 5, 6]\n",
      "        expected = [-3, 6, -3]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_with_zero_vector(self):\n",
      "        a = [0, 0, 0]\n",
      "        b = [1, 2, 3]\n",
      "        expected = [0, 0, 0]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_orthogonal_vectors(self):\n",
      "        a = [1, 0, 0]\n",
      "        b = [0, 1, 0]\n",
      "        expected = [0, 0, 1]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_parallel_vectors(self):\n",
      "        a = [2, 4, 6]\n",
      "        b = [1, 2, 3]\n",
      "        expected = [0, 0, 0]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_negative_vector(self):\n",
      "        a = [-1, 0, 0]\n",
      "        b = [0, 1, 0]\n",
      "        expected = [0, 0, -1]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertEqual(result, expected)\n",
      "\n",
      "    def test_cross_product_return_type(self):\n",
      "        a = (1, 2, 3)\n",
      "        b = [4, 5, 6]\n",
      "        result = cross_product(a, b)\n",
      "        self.assertIsInstance(result, list)\n",
      "        self.assertEqual(len(result), 3)\n",
      "\n",
      "    def test_cross_product_invalid_length(self):\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([1, 2], [3, 4, 5])\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([1, 2, 3], [4, 5])\n",
      "        with self.assertRaises(ValueError):\n",
      "            cross_product([], [])\n",
      "\n",
      "    def test_cross_product_non_sequence_inputs(self):\n",
      "        with self.assertRaises(TypeError):\n",
      "            cross_product([1, 2, 3], None)\n",
      "        with self.assertRaises(TypeError):\n",
      "            cross_product(None, [1, 2, 3])\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    unittest.main() test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.\n",
      "-----\n",
      "```json\n",
      "{\n",
      "  \"Missing_Dependency_Integration_Tests\": {\n",
      "    \"description\": \"The test suite would lack any tests that exercise interaction with external modules, services, or global state, because the tester did not consider Dependencies (D) when generating test cases.\",\n",
      "    \"rationale\": [\n",
      "      \"In the causal graph, D has a direct edge to CodeGen (D → CodeGen).\",\n",
      "      \"CodeGen is the sole source of TestCaseGen, so all test cases ultimately inherit whatever design decisions were made during code generation.\",\n",
      "      \"When D is ignored, the edge D → CodeGen is effectively severed, meaning no code is generated that mocks or verifies external interactions.\",\n",
      "      \"Thus, the generated tests will not contain any dependency‑related assertions (e.g., verifying that the function can be called with a mocked module or that it correctly handles missing external state).\",\n",
      "      \"Consequently, the likelihood of missing such integration tests is high.\"\n",
      "    ],\n",
      "    \"probability\": 0.85\n",
      "  },\n",
      "  \"Reduced_Documentation_of_External_Contract\": {\n",
      "    \"description\": \"The function’s docstring or contract will omit details about required external dependencies or the assumptions they impose, because Documentation/Contract (E) is downstream of Error Handling and Dependencies via the edges C → E and D → E.\",\n",
      "    \"rationale\": [\n",
      "      \"The graph contains D → E, indicating that awareness of dependencies informs the clarity of the contract.\",\n",
      "      \"If D is not considered, this causal influence is cut, so the generated documentation receives less signal about external requirements.\",\n",
      "      \"Since E influences CodeGen (E → CodeGen), a weaker contract leads to fewer or less detailed test assertions concerning dependency usage.\",\n",
      "      \"This results in a docstring that may not mention, for example, that the function must be called in an environment where certain global variables exist.\",\n",
      "      \"The probability of this outcome is moderate, as some documentation may still be generated from other sources (e.g., Function Complexity).\"\n",
      "    ],\n",
      "    \"probability\": 0.60\n",
      "  },\n",
      "  \"Weaker_Edge_Case_Coverage_related_to_Dependencies\": {\n",
      "    \"description\": \"Edge‑case scenarios that involve external state (such as empty or malformed dependency objects) would be less thoroughly tested, because Edge Cases (F) is influenced by D and F feeds into CodeGen.\",\n",
      "    \"rationale\": [\n",
      "      \"The causal graph includes D → F, indicating that dependencies can create new edge cases.\",\n",
      "      \"Edge Cases also influence CodeGen (F → CodeGen); without D, the downstream path that would typically enrich the code generator with defensive checks is shortened.\",\n",
      "      \"Some edge‑case tests might still arise from Function Complexity or Dependencies via other paths, but many dependency‑specific edge cases would not be produced.\",\n",
      "      \"Therefore, the test generator is likely to produce fewer or weaker tests for scenarios like a dependency that is missing required attributes or that returns unexpected values.\",\n",
      "      \"This outcome is less likely than missing integration tests but still plausible.\"\n",
      "    ],\n",
      "    \"probability\": 0.40\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "chat_history_2 = []\n",
    "last_response = ''\n",
    "messagges = copy.deepcopy(messages_2)\n",
    "input_file= 'ollama_gpt-oss:latest_step1_step2_sptep3_15.json'\n",
    "introspection_messaages =json.load(open(\"output/introspection/\"+input_file))['scenario_a']['step2']['messages']\n",
    "\n",
    "func_name, func, nodes, factors_message = process_introspection_factors(introspection_messaages)\n",
    "\n",
    "\n",
    "first_node = nodes.pop(0)\n",
    "last_message = {}\n",
    "def run(message, node):\n",
    "    \n",
    "    message['content'] = process_messages(message, None, factors_message, node, func)\n",
    "    print(message['content'])\n",
    "    print(\"-----\")\n",
    "    chat_history_2.append(message)\n",
    "    last_response = llm.call(messages=chat_history_2)\n",
    "    #print({'role':'assistant','content':last_response})\n",
    "    print(last_response)\n",
    "    chat_history_2.append({\"role\": \"assistant\", \"content\": last_response})\n",
    "\n",
    "for message in messagges:\n",
    "    last_message = copy.deepcopy(message)\n",
    "    run(message , first_node)\n",
    "\n",
    "-while nodes:\n",
    "    print(last_message)\n",
    "    run(copy.deepcopy(last_message), nodes.pop(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0dc0f",
   "metadata": {},
   "source": [
    "chat_history_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4963aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an expert in causal inference and counterfactual reasoning. Your job is to infer a causal graph from a set of variables and draft counterfactual explanations.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'I’m ready to help!  \\nTo infer a causal graph and draft counterfactual explanations, I’ll need a bit more information from you:\\n\\n1. **List of Variables** – What are the observed variables (e.g., treatment, outcome, covariates)?\\n2. **Any Known Relations** – Are there any constraints or known causal relationships you already suspect (e.g., “X causes Y”, “Z is a confounder”)?\\n3. **Data or Context** – Do you have a dataset, or is this a purely theoretical scenario? If you have data, what format is it in?\\n\\nOnce I have those details, I can:\\n\\n- Build a directed acyclic graph (DAG) that captures the causal structure.\\n- Identify potential confounders, mediators, and colliders.\\n- Construct counterfactual explanations (e.g., “If X were set to …, what would Y be?”).\\n\\nFeel free to share the variables and any preliminary insights you have!'},\n",
       " {'role': 'user',\n",
       "  'content': '[CAUSAL TASK] Giving the following variables in the json file {\\n  \"A\": {\\n    \"name\": \"Function Complexity\",\\n    \"description\": \"The number of logical branches, nested conditions, and overall algorithmic complexity of the function.\",\\n    \"weight\": 0.25\\n  },\\n  \"B\": {\\n    \"name\": \"Input Validation\",\\n    \"description\": \"Whether the function checks the validity of its inputs (e.g., length, type, value ranges).\",\\n    \"weight\": 0.20\\n  },\\n  \"C\": {\\n    \"name\": \"Error Handling\",\\n    \"description\": \"The presence and robustness of error handling logic, such as raising exceptions for invalid cases.\",\\n    \"weight\": 0.15\\n  },\\n  \"D\": {\\n    \"name\": \"Dependencies\",\\n    \"description\": \"External modules, services, or global state that the function relies on, influencing isolation in tests.\",\\n    \"weight\": 0.10\\n  },\\n  \"E\": {\\n    \"name\": \"Documentation/Contract\",\\n    \"description\": \"Clarity of the function\\'s docstring, parameter and return type contracts, and adherence to specifications.\",\\n    \"weight\": 0.15\\n  },\\n  \"F\": {\\n    \"name\": \"Edge Cases\",\\n    \"description\": \"The likelihood of encountering unusual or boundary inputs that could reveal hidden bugs.\",\\n    \"weight\": 0.15\\n  }\\n}, generate a causal graph that explains how they influence each other. Consider that those variables are influencing the generation of the test case import unittest\\nfrom typing import Sequence, List\\n\\ndef cross_product(a: Sequence[float], b: Sequence[float]) -> List[float]:\\n    \"\"\"\\n    Compute the cross product of two 3‑D vectors.\\n    \"\"\"\\n    if len(a) != 3 or len(b) != 3:\\n        raise ValueError(\"Both vectors must be of length 3.\")\\n    a1, a2, a3 = a\\n    b1, b2, b3 = b\\n    return [\\n        a2 * b3 - a3 * b2,\\n        a3 * b1 - a1 * b3,\\n        a1 * b2 - a2 * b1,\\n    ]\\n\\nclass TestCrossProduct(unittest.TestCase):\\n    def test_basic_cross_product(self):\\n        a = [1, 2, 3]\\n        b = [4, 5, 6]\\n        expected = [-3, 6, -3]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_with_zero_vector(self):\\n        a = [0, 0, 0]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_orthogonal_vectors(self):\\n        a = [1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, 1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_parallel_vectors(self):\\n        a = [2, 4, 6]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_negative_vector(self):\\n        a = [-1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, -1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_return_type(self):\\n        a = (1, 2, 3)\\n        b = [4, 5, 6]\\n        result = cross_product(a, b)\\n        self.assertIsInstance(result, list)\\n        self.assertEqual(len(result), 3)\\n\\n    def test_cross_product_invalid_length(self):\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2], [3, 4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2, 3], [4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([], [])\\n\\n    def test_cross_product_non_sequence_inputs(self):\\n        with self.assertRaises(TypeError):\\n            cross_product([1, 2, 3], None)\\n        with self.assertRaises(TypeError):\\n            cross_product(None, [1, 2, 3])\\n\\nif __name__ == \"__main__\":\\n    unittest.main(), therefore the code generation should be a separate node in the graph. The outcome must be in Graphviz DOT format.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Below is a concise causal graph in Graphviz **DOT** format that captures the inter‑relationships among the six listed variables and how they collectively drive the **Code Generation** node (i.e., the creation of the unit‑test suite for `cross_product`).  \\n\\n```dot\\ndigraph CrossProductTestGeneration {\\n    // ----- Node definitions -------------------------------------------------\\n    A [label=\"A: Function Complexity\"];\\n    B [label=\"B: Input Validation\"];\\n    C [label=\"C: Error Handling\"];\\n    D [label=\"D: Dependencies\"];\\n    E [label=\"E: Documentation/Contract\"];\\n    F [label=\"F: Edge Cases\"];\\n    CG [label=\"Code Generation\\\\n(creates test cases)\"];\\n\\n    // ----- Causal edges -----------------------------------------------------\\n    // Complexity → validation, error handling, dependencies\\n    A -> B;\\n    A -> C;\\n    A -> D;\\n\\n    // Validation → error handling\\n    B -> C;\\n\\n    // Dependencies → error handling\\n    D -> C;\\n\\n    // Documentation → validation\\n    E -> B;\\n\\n    // Edge cases → validation, error handling\\n    F -> B;\\n    F -> C;\\n\\n    // All variables → Code Generation\\n    A -> CG;\\n    B -> CG;\\n    C -> CG;\\n    D -> CG;\\n    E -> CG;\\n    F -> CG;\\n}\\n```\\n\\n**Explanation of the causal links**\\n\\n| Edge | Intuition |\\n|------|-----------|\\n| **A → B** | A more complex algorithm usually demands stricter input validation to guard against misuse. |\\n| **A → C** | Complex logic is more prone to runtime failures, so robust error handling is required. |\\n| **A → D** | Greater complexity often entails pulling in more external modules or global state. |\\n| **B → C** | If inputs aren’t validated, the function will raise errors that must be caught or handled. |\\n| **D → C** | External dependencies may throw exceptions that need to be dealt with inside the function. |\\n| **E → B** | Clear contracts (docstrings, type hints) prompt developers to validate inputs explicitly. |\\n| **F → B** | Edge‑case inputs (boundary values, unusual shapes) necessitate additional validation logic. |\\n| **F → C** | Edge cases frequently trigger error‑handling paths, so their presence informs the design of those paths. |\\n| **All → CG** | Each of the six factors provides information that guides how test cases should be written (e.g., what edge cases to cover, what error scenarios to test, how many modules to mock, etc.). |\\n\\nFeel free to feed this DOT string into Graphviz or any compatible renderer to visualise the causal structure.'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable Function Complexity during the generation of the import unittest\\nfrom typing import Sequence, List\\n\\ndef cross_product(a: Sequence[float], b: Sequence[float]) -> List[float]:\\n    \"\"\"\\n    Compute the cross product of two 3‑D vectors.\\n    \"\"\"\\n    if len(a) != 3 or len(b) != 3:\\n        raise ValueError(\"Both vectors must be of length 3.\")\\n    a1, a2, a3 = a\\n    b1, b2, b3 = b\\n    return [\\n        a2 * b3 - a3 * b2,\\n        a3 * b1 - a1 * b3,\\n        a1 * b2 - a2 * b1,\\n    ]\\n\\nclass TestCrossProduct(unittest.TestCase):\\n    def test_basic_cross_product(self):\\n        a = [1, 2, 3]\\n        b = [4, 5, 6]\\n        expected = [-3, 6, -3]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_with_zero_vector(self):\\n        a = [0, 0, 0]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_orthogonal_vectors(self):\\n        a = [1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, 1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_parallel_vectors(self):\\n        a = [2, 4, 6]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_negative_vector(self):\\n        a = [-1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, -1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_return_type(self):\\n        a = (1, 2, 3)\\n        b = [4, 5, 6]\\n        result = cross_product(a, b)\\n        self.assertIsInstance(result, list)\\n        self.assertEqual(len(result), 3)\\n\\n    def test_cross_product_invalid_length(self):\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2], [3, 4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2, 3], [4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([], [])\\n\\n    def test_cross_product_non_sequence_inputs(self):\\n        with self.assertRaises(TypeError):\\n            cross_product([1, 2, 3], None)\\n        with self.assertRaises(TypeError):\\n            cross_product(None, [1, 2, 3])\\n\\nif __name__ == \"__main__\":\\n    unittest.main() test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```json\\n{\\n  \"MissingErrorHandlingTests\": {\\n    \"description\": \"Without taking Function Complexity into account, the tester is less likely to include tests that exercise the function’s error‑handling logic (e.g., ValueError for invalid lengths, TypeError for non‑sequence inputs).\",\\n    \"rationale\": [\\n      \"The causal graph shows A (Function Complexity) → C (Error Handling).\",\\n      \"Ignoring A removes this causal influence, so the tester’s attention to error‑handling diminishes.\",\\n      \"Since error‑handling is a direct downstream effect of complexity, the probability that such tests are omitted rises.\",\\n      \"This omission is also fed into CG (Code Generation) via the edge A → CG, further reinforcing the gap.\"\\n    ],\\n    \"probability\": 0.72\\n  },\\n  \"IncompleteDependencyMocking\": {\\n    \"description\": \"If Function Complexity is not considered, the tester may fail to mock external dependencies or global state, leading to tests that hit the real dependencies instead of isolated stubs.\",\\n    \"rationale\": [\\n      \"A → D (Dependencies) indicates that a complex function usually relies on external modules.\",\\n      \"Neglecting A weakens the link to D, so the tester may not anticipate or isolate these dependencies.\",\\n      \"The downstream effect on CG (Code Generation) via A → CG implies fewer tests that explicitly mock or stub dependencies.\",\\n      \"Hence the likelihood of incomplete dependency mocking rises.\"\\n    ],\\n    \"probability\": 0.58\\n  },\\n  \"Under‑specifiedInputValidationTests\": {\\n    \"description\": \"The tester might produce too few tests for input validation, such as missing checks for vector length or type, because Function Complexity was omitted from consideration.\",\\n    \"rationale\": [\\n      \"The graph contains A → B (Input Validation).\",\\n      \"If A is ignored, the causal push toward thorough input‑validation tests is weakened.\",\\n      \"Additionally, B → C (validation influences error handling) and B → CG mean that fewer validation tests also reduce the generation of related error‑handling tests.\",\\n      \"Thus the chance that the test suite under‑specifies input validation is elevated.\"\\n    ],\\n    \"probability\": 0.45\\n  }\\n}\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable Input Validation during the generation of the import unittest\\nfrom typing import Sequence, List\\n\\ndef cross_product(a: Sequence[float], b: Sequence[float]) -> List[float]:\\n    \"\"\"\\n    Compute the cross product of two 3‑D vectors.\\n    \"\"\"\\n    if len(a) != 3 or len(b) != 3:\\n        raise ValueError(\"Both vectors must be of length 3.\")\\n    a1, a2, a3 = a\\n    b1, b2, b3 = b\\n    return [\\n        a2 * b3 - a3 * b2,\\n        a3 * b1 - a1 * b3,\\n        a1 * b2 - a2 * b1,\\n    ]\\n\\nclass TestCrossProduct(unittest.TestCase):\\n    def test_basic_cross_product(self):\\n        a = [1, 2, 3]\\n        b = [4, 5, 6]\\n        expected = [-3, 6, -3]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_with_zero_vector(self):\\n        a = [0, 0, 0]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_orthogonal_vectors(self):\\n        a = [1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, 1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_parallel_vectors(self):\\n        a = [2, 4, 6]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_negative_vector(self):\\n        a = [-1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, -1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_return_type(self):\\n        a = (1, 2, 3)\\n        b = [4, 5, 6]\\n        result = cross_product(a, b)\\n        self.assertIsInstance(result, list)\\n        self.assertEqual(len(result), 3)\\n\\n    def test_cross_product_invalid_length(self):\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2], [3, 4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2, 3], [4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([], [])\\n\\n    def test_cross_product_non_sequence_inputs(self):\\n        with self.assertRaises(TypeError):\\n            cross_product([1, 2, 3], None)\\n        with self.assertRaises(TypeError):\\n            cross_product(None, [1, 2, 3])\\n\\nif __name__ == \"__main__\":\\n    unittest.main() test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```json\\n{\\n  \"MissingErrorHandlingTests\": {\\n    \"description\": \"If the tester had not taken Input Validation into account, the generated unit‑test suite would likely omit the explicit tests that exercise the function’s error‑handling logic (e.g., the `ValueError` for vectors of incorrect length and the `TypeError` for non‑sequence inputs).\",\\n    \"rationale\": [\\n      \"The causal graph contains a direct edge B → C, meaning that input validation directly drives the design of error‑handling tests.\",\\n      \"Neglecting B removes that push, so the tester is less motivated to write tests that trigger the `raise ValueError` or `raise TypeError` branches.\",\\n      \"Because CG (Code Generation) is also connected to B (B → CG), dropping B weakens the overall guidance on what tests should be included, further increasing the chance that error‑handling cases are missed.\",\\n      \"Thus, the probability that such tests are omitted rises substantially.\"\\n    ],\\n    \"probability\": 0.75\\n  },\\n  \"ReducedBoundaryCoverage\": {\\n    \"description\": \"Without considering Input Validation, the tester would be less likely to cover boundary or edge‑case inputs such as zero vectors, negative components, or vectors that are just on the edge of the valid length requirement.\",\\n    \"rationale\": [\\n      \"Edge Cases (F) has edges F → B and F → C; B influences how many boundary conditions are explicitly tested.\",\\n      \"If B is ignored, the influence of F on test generation is weakened, because the tester no longer connects boundary inputs to a need for validation checks.\",\\n      \"Consequently, test cases like `test_cross_product_with_zero_vector` or `test_cross_product_negative_vector` may be omitted or under‑specified.\",\\n      \"This reduces overall coverage of boundary scenarios.\"\\n    ],\\n    \"probability\": 0.55\\n  },\\n  \"OverallTestSuiteShortfall\": {\\n    \"description\": \"The absence of Input Validation in the tester’s mental model would lead to a smaller, less comprehensive test suite overall, with fewer assertions and helper checks.\",\\n    \"rationale\": [\\n      \"The graph shows B → CG, meaning that input validation directly informs the number and variety of tests generated.\",\\n      \"Removing B weakens this link, so the causal flow to the Code Generation node is less strong and fewer tests are produced.\",\\n      \"Additionally, B’s downstream effect on C (error handling) is also part of the test logic, so the combined impact of missing B on both CG and C results in a more sparse test suite.\",\\n      \"Therefore, the likelihood that the test suite is noticeably incomplete is moderate to high.\"\\n    ],\\n    \"probability\": 0.60\\n  }\\n}\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable Input Validation during the generation of the import unittest\\nfrom typing import Sequence, List\\n\\ndef cross_product(a: Sequence[float], b: Sequence[float]) -> List[float]:\\n    \"\"\"\\n    Compute the cross product of two 3‑D vectors.\\n    \"\"\"\\n    if len(a) != 3 or len(b) != 3:\\n        raise ValueError(\"Both vectors must be of length 3.\")\\n    a1, a2, a3 = a\\n    b1, b2, b3 = b\\n    return [\\n        a2 * b3 - a3 * b2,\\n        a3 * b1 - a1 * b3,\\n        a1 * b2 - a2 * b1,\\n    ]\\n\\nclass TestCrossProduct(unittest.TestCase):\\n    def test_basic_cross_product(self):\\n        a = [1, 2, 3]\\n        b = [4, 5, 6]\\n        expected = [-3, 6, -3]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_with_zero_vector(self):\\n        a = [0, 0, 0]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_orthogonal_vectors(self):\\n        a = [1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, 1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_parallel_vectors(self):\\n        a = [2, 4, 6]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_negative_vector(self):\\n        a = [-1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, -1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_return_type(self):\\n        a = (1, 2, 3)\\n        b = [4, 5, 6]\\n        result = cross_product(a, b)\\n        self.assertIsInstance(result, list)\\n        self.assertEqual(len(result), 3)\\n\\n    def test_cross_product_invalid_length(self):\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2], [3, 4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2, 3], [4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([], [])\\n\\n    def test_cross_product_non_sequence_inputs(self):\\n        with self.assertRaises(TypeError):\\n            cross_product([1, 2, 3], None)\\n        with self.assertRaises(TypeError):\\n            cross_product(None, [1, 2, 3])\\n\\nif __name__ == \"__main__\":\\n    unittest.main() test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```json\\n{\\n  \"MissingErrorHandlingTests\": {\\n    \"description\": \"The tester would likely omit explicit tests for the ValueError and TypeError branches because Input Validation (B) directly drives the design of error‑handling tests (B → C).\",\\n    \"rationale\": [\\n      \"In the causal graph, B has a direct edge to C, indicating that proper input validation motivates the creation of error‑handling test cases.\",\\n      \"If B is not considered, that causal influence is removed, weakening the motivation to exercise the `raise ValueError` and `raise TypeError` paths.\",\\n      \"CG (Code Generation) also receives input from B (B → CG), so dropping B reduces the overall guidance on which tests to include.\",\\n      \"Consequently, the probability that such error‑handling tests are omitted rises significantly.\"\\n    ],\\n    \"probability\": 0.82\\n  },\\n  \"ReducedBoundaryCoverage\": {\\n    \"description\": \"Boundary or edge‑case vectors (e.g., zero vectors, negative components, or vectors with length just off the valid bound) would be under‑tested, as Input Validation normally mediates the link between Edge Cases (F) and test coverage (F → B).\",\\n    \"rationale\": [\\n      \"The graph shows edges F → B and F → C, meaning that edge‑case inputs motivate both input validation checks and error‑handling logic.\",\\n      \"Neglecting B breaks the causal flow from F to B, so the tester is less likely to write tests that cover these edge conditions.\",\\n      \"This results in a noticeable drop in boundary coverage, even though the function still has tests for orthogonal, parallel, or negative vectors.\"\\n    ],\\n    \"probability\": 0.58\\n  },\\n  \"OverallTestSuiteShortfall\": {\\n    \"description\": \"The overall test suite would be smaller and less comprehensive because the causal link from Input Validation to Code Generation (B → CG) is missing, reducing the number of generated test cases.\",\\n    \"rationale\": [\\n      \"CG is the node that aggregates all causal inputs to produce the test suite.\",\\n      \"With B removed, the graph loses a direct influence on CG, weakening the stimulus for test creation.\",\\n      \"Since B also indirectly influences C, which in turn affects CG (through error‑handling tests), the combined effect further reduces test coverage.\",\\n      \"Thus, there is a moderate to high probability that the final test set is noticeably incomplete.\"\\n    ],\\n    \"probability\": 0.61\\n  }\\n}\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable Input Validation during the generation of the import unittest\\nfrom typing import Sequence, List\\n\\ndef cross_product(a: Sequence[float], b: Sequence[float]) -> List[float]:\\n    \"\"\"\\n    Compute the cross product of two 3‑D vectors.\\n    \"\"\"\\n    if len(a) != 3 or len(b) != 3:\\n        raise ValueError(\"Both vectors must be of length 3.\")\\n    a1, a2, a3 = a\\n    b1, b2, b3 = b\\n    return [\\n        a2 * b3 - a3 * b2,\\n        a3 * b1 - a1 * b3,\\n        a1 * b2 - a2 * b1,\\n    ]\\n\\nclass TestCrossProduct(unittest.TestCase):\\n    def test_basic_cross_product(self):\\n        a = [1, 2, 3]\\n        b = [4, 5, 6]\\n        expected = [-3, 6, -3]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_with_zero_vector(self):\\n        a = [0, 0, 0]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_orthogonal_vectors(self):\\n        a = [1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, 1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_parallel_vectors(self):\\n        a = [2, 4, 6]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_negative_vector(self):\\n        a = [-1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, -1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_return_type(self):\\n        a = (1, 2, 3)\\n        b = [4, 5, 6]\\n        result = cross_product(a, b)\\n        self.assertIsInstance(result, list)\\n        self.assertEqual(len(result), 3)\\n\\n    def test_cross_product_invalid_length(self):\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2], [3, 4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2, 3], [4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([], [])\\n\\n    def test_cross_product_non_sequence_inputs(self):\\n        with self.assertRaises(TypeError):\\n            cross_product([1, 2, 3], None)\\n        with self.assertRaises(TypeError):\\n            cross_product(None, [1, 2, 3])\\n\\nif __name__ == \"__main__\":\\n    unittest.main() test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```json\\n{\\n  \"MissingErrorHandlingTests\": {\\n    \"description\": \"If the tester did not account for Input Validation (B), the generated unit‑test suite would likely omit explicit checks for the ValueError raised when vector lengths differ and the TypeError raised for non‑sequence inputs.\",\\n    \"rationale\": [\\n      \"The causal graph contains a direct edge B → C, indicating that proper input validation drives the creation of error‑handling tests.\",\\n      \"When B is ignored, this causal influence disappears, so the motivation to write tests that exercise the raise ValueError and raise TypeError branches weakens.\",\\n      \"Because CG (Code Generation) also receives input from B (B → CG), dropping B further reduces the overall guidance on which tests should be included.\",\\n      \"Consequently, the probability that these error‑handling tests are omitted rises substantially.\"\\n    ],\\n    \"probability\": 0.82\\n  },\\n  \"ReducedBoundaryCoverage\": {\\n    \"description\": \"Without considering Input Validation, the tester would be less likely to cover boundary or edge‑case vectors such as zero vectors, negative components, or vectors that are just outside the valid length, because the link between Edge Cases (F) and test coverage (F → B) is weakened.\",\\n    \"rationale\": [\\n      \"The graph shows edges F → B and F → C, meaning that edge‑case inputs motivate both input validation checks and error‑handling logic.\",\\n      \"Neglecting B breaks the causal flow from F to B, so the tester is less inclined to write tests for these edge conditions.\",\\n      \"As a result, boundary coverage would be noticeably lower even though the function still has tests for orthogonal, parallel, or negative vectors.\"\\n    ],\\n    \"probability\": 0.58\\n  },\\n  \"OverallTestSuiteShortfall\": {\\n    \"description\": \"The overall test suite would be smaller and less comprehensive because the causal link from Input Validation to Code Generation (B → CG) is missing, reducing the number of generated test cases.\",\\n    \"rationale\": [\\n      \"CG aggregates all causal inputs to produce the test suite.\",\\n      \"With B removed, the graph loses a direct influence on CG, weakening the stimulus for test creation.\",\\n      \"Since B also indirectly influences C, which in turn affects CG through error‑handling tests, the combined effect further shrinks test coverage.\",\\n      \"Thus, there is a moderate to high probability that the final test set is noticeably incomplete.\"\\n    ],\\n    \"probability\": 0.61\\n  }\\n}\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable Input Validation during the generation of the import unittest\\nfrom typing import Sequence, List\\n\\ndef cross_product(a: Sequence[float], b: Sequence[float]) -> List[float]:\\n    \"\"\"\\n    Compute the cross product of two 3‑D vectors.\\n    \"\"\"\\n    if len(a) != 3 or len(b) != 3:\\n        raise ValueError(\"Both vectors must be of length 3.\")\\n    a1, a2, a3 = a\\n    b1, b2, b3 = b\\n    return [\\n        a2 * b3 - a3 * b2,\\n        a3 * b1 - a1 * b3,\\n        a1 * b2 - a2 * b1,\\n    ]\\n\\nclass TestCrossProduct(unittest.TestCase):\\n    def test_basic_cross_product(self):\\n        a = [1, 2, 3]\\n        b = [4, 5, 6]\\n        expected = [-3, 6, -3]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_with_zero_vector(self):\\n        a = [0, 0, 0]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_orthogonal_vectors(self):\\n        a = [1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, 1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_parallel_vectors(self):\\n        a = [2, 4, 6]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_negative_vector(self):\\n        a = [-1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, -1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_return_type(self):\\n        a = (1, 2, 3)\\n        b = [4, 5, 6]\\n        result = cross_product(a, b)\\n        self.assertIsInstance(result, list)\\n        self.assertEqual(len(result), 3)\\n\\n    def test_cross_product_invalid_length(self):\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2], [3, 4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2, 3], [4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([], [])\\n\\n    def test_cross_product_non_sequence_inputs(self):\\n        with self.assertRaises(TypeError):\\n            cross_product([1, 2, 3], None)\\n        with self.assertRaises(TypeError):\\n            cross_product(None, [1, 2, 3])\\n\\nif __name__ == \"__main__\":\\n    unittest.main() test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```json\\n{\\n  \"OmittedErrorHandlingTests\": {\\n    \"description\": \"The test suite would likely lack explicit checks for the `ValueError` raised on mismatched vector lengths and the `TypeError` raised when a non‑sequence is supplied, because Input Validation (B) directly drives the creation of such error‑handling tests (B → C) and feeds into Code Generation (B → CG).\",\\n    \"rationale\": [\\n      \"1️⃣ In the causal graph, B has a direct arrow to C, signalling that proper input validation motivates writing test cases that exercise the error‑handling logic.\",\\n      \"2️⃣ When B is not considered, the causal link B → C disappears, weakening the impetus to cover the `raise ValueError` and `raise TypeError` branches.\",\\n      \"3️⃣ CG also receives input from B (B → CG); removing B reduces the overall guidance on which tests must be generated.\",\\n      \"4️⃣ Consequently, the probability that these error‑handling tests are omitted rises dramatically.\"\\n    ],\\n    \"probability\": 0.92\\n  },\\n  \"Under‑TestedBoundaryVectors\": {\\n    \"description\": \"Boundary or edge‑case vectors (e.g., zero vectors, negative components, or lengths just outside the valid bound) would receive less coverage, because the link between Edge Cases (F) and Input Validation (F → B) would be broken, weakening the motivation to write tests for such inputs.\",\\n    \"rationale\": [\\n      \"1️⃣ The graph shows edges F → B and F → C, meaning edge‑case inputs normally prompt both validation checks and error‑handling logic.\",\\n      \"2️⃣ Without considering B, the causal flow from F to B is severed, so edge‑case inputs no longer drive the creation of validation tests.\",\\n      \"3️⃣ As a result, the tester is less inclined to write tests that cover zero or negative vectors, reducing boundary coverage.\",\\n      \"4️⃣ This effect is indirect but still substantial, increasing the likelihood of incomplete boundary testing.\"\\n    ],\\n    \"probability\": 0.65\\n  },\\n  \"DiminishedOverallTestCoverage\": {\\n    \"description\": \"The final test suite would be noticeably smaller and less comprehensive, because the direct influence of Input Validation on Code Generation (B → CG) is missing, weakening the stimulus for generating multiple test cases.\",\\n    \"rationale\": [\\n      \"1️⃣ CG aggregates all causal inputs to produce the test suite; B is a direct driver (B → CG).\",\\n      \"2️⃣ Removing B weakens the flow to CG, leading to fewer test cases being produced.\",\\n      \"3️⃣ B also indirectly influences C, which in turn affects CG via error‑handling tests; losing B removes both direct and indirect influences on CG.\",\\n      \"4️⃣ Therefore, the overall probability of a noticeable test suite shortfall is moderate to high.\"\\n    ],\\n    \"probability\": 0.55\\n  }\\n}\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '[COUNTERFACTUAL TASK] What would have changed or happened if the software tester had not considered the variable Input Validation during the generation of the import unittest\\nfrom typing import Sequence, List\\n\\ndef cross_product(a: Sequence[float], b: Sequence[float]) -> List[float]:\\n    \"\"\"\\n    Compute the cross product of two 3‑D vectors.\\n    \"\"\"\\n    if len(a) != 3 or len(b) != 3:\\n        raise ValueError(\"Both vectors must be of length 3.\")\\n    a1, a2, a3 = a\\n    b1, b2, b3 = b\\n    return [\\n        a2 * b3 - a3 * b2,\\n        a3 * b1 - a1 * b3,\\n        a1 * b2 - a2 * b1,\\n    ]\\n\\nclass TestCrossProduct(unittest.TestCase):\\n    def test_basic_cross_product(self):\\n        a = [1, 2, 3]\\n        b = [4, 5, 6]\\n        expected = [-3, 6, -3]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_with_zero_vector(self):\\n        a = [0, 0, 0]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_orthogonal_vectors(self):\\n        a = [1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, 1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_parallel_vectors(self):\\n        a = [2, 4, 6]\\n        b = [1, 2, 3]\\n        expected = [0, 0, 0]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_negative_vector(self):\\n        a = [-1, 0, 0]\\n        b = [0, 1, 0]\\n        expected = [0, 0, -1]\\n        result = cross_product(a, b)\\n        self.assertEqual(result, expected)\\n\\n    def test_cross_product_return_type(self):\\n        a = (1, 2, 3)\\n        b = [4, 5, 6]\\n        result = cross_product(a, b)\\n        self.assertIsInstance(result, list)\\n        self.assertEqual(len(result), 3)\\n\\n    def test_cross_product_invalid_length(self):\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2], [3, 4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([1, 2, 3], [4, 5])\\n        with self.assertRaises(ValueError):\\n            cross_product([], [])\\n\\n    def test_cross_product_non_sequence_inputs(self):\\n        with self.assertRaises(TypeError):\\n            cross_product([1, 2, 3], None)\\n        with self.assertRaises(TypeError):\\n            cross_product(None, [1, 2, 3])\\n\\nif __name__ == \"__main__\":\\n    unittest.main() test? Provide at most three counterfactual answers. Estimate a probability of occurrence for each answer based solely on the causal graph in Graphviz DOT format. The outcome must be contained in a json format in which the first level comprises the name of the counterfactual answer; the second level comprises a description of the answer, a step by step of the rationale of that counterfactual answer, and the probability of occurrence.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```json\\n{\\n  \"OmittedErrorHandlingTests\": {\\n    \"description\": \"The generated test suite would likely miss explicit checks for the `ValueError` triggered by length mismatches and the `TypeError` triggered by non‑sequence inputs, because Input Validation (B) directly drives error‑handling test creation (B → C) and also feeds into Code Generation (B → CG).\",\\n    \"rationale\": [\\n      \"1. The causal graph contains a direct arrow B → C, indicating that proper input validation motivates writing tests that exercise the error‑handling logic.\",\\n      \"2. Ignoring B removes this arrow, so the motivation to cover the `raise ValueError` and `raise TypeError` branches weakens.\",\\n      \"3. CG receives input from B (B → CG); dropping B further reduces the overall guidance on which test cases to include.\",\\n      \"4. Consequently, the likelihood that these error‑handling tests are omitted increases substantially.\"\\n    ],\\n    \"probability\": 0.88\\n  },\\n  \"ReducedBoundaryVectorCoverage\": {\\n    \"description\": \"Without considering Input Validation, boundary or edge‑case vectors (e.g., zero vectors, negative components, or lengths just outside the valid bound) would receive less coverage, because the link between Edge Cases (F) and Input Validation (F → B) is broken, weakening the incentive to write tests for such inputs.\",\\n    \"rationale\": [\\n      \"1. The graph shows edges F → B and F → C, meaning edge‑case inputs normally prompt both validation checks and error‑handling logic.\",\\n      \"2. Ignoring B severs the causal flow from F to B, so edge‑case inputs no longer drive the creation of validation tests.\",\\n      \"3. As a result, the tester is less inclined to write tests covering zero or negative vectors, leading to diminished boundary coverage.\",\\n      \"4. This indirect effect still significantly raises the probability of incomplete boundary testing.\"\\n    ],\\n    \"probability\": 0.63\\n  },\\n  \"OverallTestSuiteContraction\": {\\n    \"description\": \"The final test suite would be noticeably smaller and less comprehensive, because Input Validation is a direct driver of Code Generation (B → CG); removing B weakens the stimulus for producing multiple test cases.\",\\n    \"rationale\": [\\n      \"1. CG aggregates all causal inputs to produce the test suite; B is a direct driver (B → CG).\",\\n      \"2. Removing B weakens the flow to CG, leading to fewer test cases being generated.\",\\n      \"3. B also indirectly influences C, which in turn affects CG via error‑handling tests; losing B removes both direct and indirect influences on CG.\",\\n      \"4. Therefore, the overall probability of a noticeable shortfall in test coverage is moderate to high.\"\\n    ],\\n    \"probability\": 0.55\\n  }\\n}\\n```'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad94d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
